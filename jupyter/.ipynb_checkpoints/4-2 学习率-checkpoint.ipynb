{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate = 0.2\n",
    "\n",
    "w = tf.Variable(tf.constant(5, dtype=tf.float32))\n",
    "loss = tf.square(w+1)\n",
    "opt = tf.train.GradientDescentOptimizer(learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 step: w is 2.600000, loss is 12.959999 \n",
      "After 1 step: w is 1.160000, loss is 4.665599 \n",
      "After 2 step: w is 0.296000, loss is 1.679616 \n",
      "After 3 step: w is -0.222400, loss is 0.604662 \n",
      "After 4 step: w is -0.533440, loss is 0.217678 \n",
      "After 5 step: w is -0.720064, loss is 0.078364 \n",
      "After 6 step: w is -0.832038, loss is 0.028211 \n",
      "After 7 step: w is -0.899223, loss is 0.010156 \n",
      "After 8 step: w is -0.939534, loss is 0.003656 \n",
      "After 9 step: w is -0.963720, loss is 0.001316 \n",
      "After 10 step: w is -0.978232, loss is 0.000474 \n",
      "After 11 step: w is -0.986939, loss is 0.000171 \n",
      "After 12 step: w is -0.992164, loss is 0.000061 \n",
      "After 13 step: w is -0.995298, loss is 0.000022 \n",
      "After 14 step: w is -0.997179, loss is 0.000008 \n",
      "After 15 step: w is -0.998307, loss is 0.000003 \n",
      "After 16 step: w is -0.998984, loss is 0.000001 \n",
      "After 17 step: w is -0.999391, loss is 0.000000 \n",
      "After 18 step: w is -0.999634, loss is 0.000000 \n",
      "After 19 step: w is -0.999781, loss is 0.000000 \n",
      "After 20 step: w is -0.999868, loss is 0.000000 \n",
      "After 21 step: w is -0.999921, loss is 0.000000 \n",
      "After 22 step: w is -0.999953, loss is 0.000000 \n",
      "After 23 step: w is -0.999972, loss is 0.000000 \n",
      "After 24 step: w is -0.999983, loss is 0.000000 \n",
      "After 25 step: w is -0.999990, loss is 0.000000 \n",
      "After 26 step: w is -0.999994, loss is 0.000000 \n",
      "After 27 step: w is -0.999996, loss is 0.000000 \n",
      "After 28 step: w is -0.999998, loss is 0.000000 \n",
      "After 29 step: w is -0.999999, loss is 0.000000 \n",
      "After 30 step: w is -0.999999, loss is 0.000000 \n",
      "After 31 step: w is -1.000000, loss is 0.000000 \n",
      "After 32 step: w is -1.000000, loss is 0.000000 \n",
      "After 33 step: w is -1.000000, loss is 0.000000 \n",
      "After 34 step: w is -1.000000, loss is 0.000000 \n",
      "After 35 step: w is -1.000000, loss is 0.000000 \n",
      "After 36 step: w is -1.000000, loss is 0.000000 \n",
      "After 37 step: w is -1.000000, loss is 0.000000 \n",
      "After 38 step: w is -1.000000, loss is 0.000000 \n",
      "After 39 step: w is -1.000000, loss is 0.000000 \n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for i in range(40):\n",
    "        sess.run(train_step)\n",
    "        w_val = sess.run(w)\n",
    "        loss_val = sess.run(loss)\n",
    "        print(\"After %s step: w is %f, loss is %f \" %(i,w_val,loss_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 指数衰减学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARN_RATE_BASE = 0.1 #最初的学习率\n",
    "LEARN_RATE_DECAY = 0.99 #学习率衰减率\n",
    "LEARN_RATE_STEP = 1 #喂入多少轮BATCH_SIZE后，更新一次学习率，一般设为：总体样本数/BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#运行了几轮BATCH_SIZE的计数器，初值为0，设为不被训练\n",
    "globle_step = tf.Variable(0, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义指数下降学习率\n",
    "learning_rate = tf.train.exponential_decay(LEARN_RATE_BASE,globle_step,/\n",
    "                                           LEARN_RATE_STEP,/\n",
    "                                           LEARN_RATE_DECAY, staircase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(tf.constant(5, dtype=tf.float32))\n",
    "loss = tf.square(w+1)\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = opt.minimize(loss,global_step=globle_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'globle_step_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-a30752474253>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mlearning_rate_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         print(\"After %s step: global_step is %f, w is %f, learing rate is %f, loss is %f \"\n\u001b[1;32m---> 11\u001b[1;33m               %(i,globle_step_val,w_val,learning_rate_val, loss_val))\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'globle_step_val' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for i in range(40):\n",
    "        sess.run(train_step)\n",
    "        w_val = sess.run(w)\n",
    "        loss_val = sess.run(loss)\n",
    "        global_step_val = sess.run(globle_step)\n",
    "        learning_rate_val = sess.run(learning_rate)\n",
    "        print(\"After %s step: global_step is %f, w is %f, learing rate is %f, loss is %f \"\n",
    "              %(i,global_step_val,w_val,learning_rate_val, loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
